{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "display(HTML(\"<style>.cell { height:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# improved BOW validation script\n",
    "# changes: leave stopwords in, use TF-IDF vectorizer, removed converting vectorizer output to np.array\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "data_file = FILEPATH\n",
    "data = pd.read_csv( data_file, header = 0, quoting = 0, encoding='utf-8', error_bad_lines=True)\n",
    "\n",
    "train_i, test_i = train_test_split( np.arange( len( data )), train_size = 0.8, random_state = 44)\n",
    "\n",
    "train = data.loc[train_i]\n",
    "test = data.loc[test_i]\n",
    "\n",
    "#\n",
    "\n",
    "print \"done\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "otherfeaturestraindf = pd.DataFrame({\n",
    "   \n",
    "        'Day':train[\"Day\"].values.tolist(), 'Month':train[\"Month\"].values.tolist(), \n",
    "        'Hour':train[\"Hour\"].values.tolist(), 'Time_Period':train[\"Time_Period\"].values.tolist(),\n",
    "        'pctwhite':train[\"pctwhite\"].values.tolist(), 'pctblack':train[\"pctblack\"].values.tolist(), 'pctapi':train[\"pctapi\"].values.tolist(),\n",
    "        'pctaian':train[\"pctaian\"].values.tolist(), 'pct2prace':train[\"pct2prace\"].values.tolist(),\n",
    "        'pcthispanic':train[\"pcthispanic\"].values.tolist(), 'gender':train[\"gender\"].values.tolist(),                              \n",
    "        'display_name_words':train[\"display_name_words\"].values.tolist(), 'display_name_len':train[\"display_name_len\"].values.tolist(),\n",
    "        'display_name_upper':train[\"display_name_upper\"].values.tolist(), 'display_name_lower':train[\"display_name_lower\"].values.tolist(),\n",
    "        'Follower_Count':train[\"Follower_Count\"].values.tolist(), 'Verified':train[\"Verified\"].values.tolist(),                                  \n",
    "        'Location':train[\"Location\"].values.tolist(), 'is_location':train[\"is_location\"].values.tolist(), 'Description':train[\"Description\"].values.tolist(), \n",
    "        'is_description':train[\"is_description\"].values.tolist(), 'Friends_Count':train[\"Friends_Count\"].values.tolist(), 'List_Count':train[\"List_Count\"].values.tolist(),                               \n",
    "        'Favourites_Count':train[\"Favourites_Count\"].values.tolist(), 'Statuses_Count':train[\"Statuses_Count\"].values.tolist(), 'Time Zone':train[\"Time_Zone\"].values.tolist(),\n",
    "        'is_timezone':train[\"is_timezone\"].values.tolist(), 'Language':train[\"Language\"].values.tolist(), 'is_Background_Image':train[\"is_Background_Image\"].values.tolist(),                              \n",
    "        'Default_Profile':train[\"Default_Profile\"].values.tolist(), 'Default_Profile_Image':train[\"Default_Profile_Image\"].values.tolist(),\n",
    "        'has_url':train[\"has_url\"].values.tolist(),\n",
    "        'Year_created':train[\"Year_created\"].values.tolist(), \n",
    "        'Profile_Background_Tile':train[\"Profile_Background_Tile\"].values.tolist(), 'Account_Status':train[\"Account_Status\"].values.tolist(),\n",
    "        'is_active':train[\"is_active\"].values.tolist(), 'username_len':train[\"username_len\"].values.tolist(), \n",
    "        'user_name_upper':train[\"user_name_upper\"].values.tolist(), 'user_name_lower':train[\"user_name_lower\"].values.tolist(), \n",
    "        'faces_detected':train[\"faces_detected\"].values.tolist(), \n",
    "        'is_geo':train[\"is_geo\"].values.tolist(),  \n",
    "        'total_misspelling':train[\"total_misspelling\"].values.tolist(), 'total_correct':train[\"total_correct\"].values.tolist(),\n",
    "        'contains_misspellings':train[\"contains_misspellings\"].values.tolist(), 'profanity_count':train[\"profanity_count\"].values.tolist(),\n",
    "        'not_profanity':train[\"not_profanity\"].values.tolist(),                               \n",
    "        'contains_profanity':train[\"contains_profanity\"].values.tolist(), 'subjectivity':train[\"subjectivity\"].values.tolist(),\n",
    "        'word_count':train[\"word_count\"].values.tolist(), 'tweet_len':train[\"tweet_len\"].values.tolist(), 'tweet_len_cat':train[\"tweet_len_cat\"].values.tolist(), \n",
    "        'period_count':train[\"period_count\"].values.tolist(), 'period_percent':train[\"period_percent\"].values.tolist(),\n",
    "        'qmark_count':train[\"qmark_count\"].values.tolist(), 'qmark_percent':train[\"qmark_percent\"].values.tolist(),                               \n",
    "        'upper_count':train[\"upper_count\"].values.tolist(), 'upper_percent':train[\"upper_percent\"].values.tolist(), \n",
    "        'lower_count':train[\"lower_count\"].values.tolist(), 'lower_percent':train[\"lower_percent\"].values.tolist(), \n",
    "        'comma_count':train[\"comma_count\"].values.tolist(), 'comma_percent':train[\"comma_percent\"].values.tolist(),                               \n",
    "        'flash_count':train[\"flash_count\"].values.tolist(), 'flash_norm':train[\"flash_norm\"].values.tolist(),                               \n",
    "        'exclamation_count':train[\"exclamation_count\"].values.tolist(), 'exclamation_percent':train[\"exclamation_percent\"].values.tolist(),                               \n",
    "        'is_url':train[\"is_url\"].values.tolist(), 'url_len':train[\"url_len\"].values.tolist(),                               \n",
    "        'hashtag_len':train[\"hashtag_len\"].values.tolist(), 'hashtag_count':train[\"hashtag_count\"].values.tolist(),                               \n",
    "        'mention_count':train[\"mention_count\"].values.tolist(), 'mention_len':train[\"mention_len\"].values.tolist(),\n",
    "        'is_reply':train[\"is_reply\"].values.tolist(), 'Overall.score.EMOTIVE':train[\"Overall.score.EMOTIVE\"].values.tolist(),\n",
    "        'Anger':train[\"Anger\"].values.tolist(),\n",
    "        'Anger_norm':train[\"Anger_norm\"].values.tolist(), 'Confusion':train[\"Confusion\"].values.tolist(),\n",
    "        'Confusion_norm':train[\"Confusion_norm\"].values.tolist(), 'Disgust':train[\"Disgust\"].values.tolist(),\n",
    "        'Disgust_norm':train[\"Disgust_norm\"].values.tolist(),\n",
    "        'Fear':train[\"Fear\"].values.tolist(), 'Fear_norm':train[\"Fear_norm\"].values.tolist(),\n",
    "        'Happiness':train[\"Happiness\"].values.tolist(), 'Happiness_norm':train[\"Happiness_norm\"].values.tolist(),\n",
    "        'Sadness':train[\"Sadness\"].values.tolist(), 'Sadness_norm':train[\"Sadness_norm\"].values.tolist(),                              \n",
    "        'Shame':train[\"Shame\"].values.tolist(), 'Shame_norm':train[\"Shame_norm\"].values.tolist(), \n",
    "        'Surprise':train[\"Surprise\"].values.tolist(), 'Surprise_norm':train[\"Surprise_norm\"].values.tolist(),                                \n",
    "        'is_Overall.score.EMOTIVE':train[\"is_Overall.score.EMOTIVE\"].values.tolist(), 'is_Anger':train[\"is_Anger\"].values.tolist(),\n",
    "        'is_Confusion':train[\"is_Confusion\"].values.tolist(), 'is_Disgust':train[\"is_Disgust\"].values.tolist(),                              \n",
    "        'is_Fear':train[\"is_Fear\"].values.tolist(), 'is_Happiness':train[\"is_Happiness\"].values.tolist(),                              \n",
    "        'is_Sadness':train[\"is_Sadness\"].values.tolist(), 'is_Shame':train[\"is_Shame\"].values.tolist(),                              \n",
    "        'is_Surprise':train[\"is_Surprise\"].values.tolist(), 'Type':train[\"Type\"].values.tolist(),                              \n",
    "        'senti_positive':train[\"senti_positive\"].values.tolist(), \n",
    "        'senti_mix':train[\"senti_mix\"].values.tolist(), 'total_senti':train[\"total_senti\"].values.tolist(),\n",
    "        'senti_positive_percent':train[\"senti_positive_percent\"].values.tolist(), 'senti_negative_percent':train[\"senti_negative_percent\"].values.tolist(),\n",
    "        'CC':train[\"CC\"].values.tolist(), 'CD':train[\"CD\"].values.tolist(), \n",
    "        'DT':train[\"DT\"].values.tolist(), 'EX':train[\"EX\"].values.tolist(), \n",
    "        'FW':train[\"FW\"].values.tolist(), 'IN':train[\"IN\"].values.tolist(), \n",
    "        'JJ':train[\"JJ\"].values.tolist(), 'JJR':train[\"JJR\"].values.tolist(), \n",
    "        'JJS':train[\"JJS\"].values.tolist(), 'MD':train[\"MD\"].values.tolist(), \n",
    "        'NN':train[\"NN\"].values.tolist(), 'NNP':train[\"NNP\"].values.tolist(), \n",
    "        'NNPS':train[\"NNPS\"].values.tolist(), 'NNS':train[\"NNS\"].values.tolist(),                               \n",
    "        'PDT':train[\"PDT\"].values.tolist(), 'POS':train[\"POS\"].values.tolist(), \n",
    "        'PRP':train[\"PRP\"].values.tolist(), 'PRP$':train[\"PRP$\"].values.tolist(),  \n",
    "        'RBR':train[\"RBR\"].values.tolist(), \n",
    "        'RBS':train[\"RBS\"].values.tolist(), 'RP':train[\"RP\"].values.tolist(),                               \n",
    "        'SYM':train[\"SYM\"].values.tolist(), 'TO':train[\"TO\"].values.tolist(), \n",
    "        'UH':train[\"UH\"].values.tolist(), 'VB':train[\"VB\"].values.tolist(),                                \n",
    "        'VBD':train[\"VBD\"].values.tolist(), 'VBG':train[\"VBG\"].values.tolist(), \n",
    "        'VBN':train[\"VBN\"].values.tolist(), 'VBP':train[\"VBP\"].values.tolist(),                               \n",
    "        'VBZ':train[\"VBZ\"].values.tolist(), 'WDT':train[\"WDT\"].values.tolist(),\n",
    "        'WP':train[\"WP\"].values.tolist(), 'WP$':train[\"WP$\"].values.tolist(), \n",
    "        'WRB':train[\"WRB\"].values.tolist(), \n",
    "                                                       \n",
    "                             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "otherfeaturestestdf = pd.DataFrame({   \n",
    "    \n",
    "        'Day':test[\"Day\"].values.tolist(), 'Month':test[\"Month\"].values.tolist(), \n",
    "        'Hour':test[\"Hour\"].values.tolist(), 'Time_Period':test[\"Time_Period\"].values.tolist(),\n",
    "        'pctwhite':test[\"pctwhite\"].values.tolist(), 'pctblack':test[\"pctblack\"].values.tolist(), 'pctapi':test[\"pctapi\"].values.tolist(),\n",
    "        'pctaian':test[\"pctaian\"].values.tolist(), 'pct2prace':test[\"pct2prace\"].values.tolist(),\n",
    "        'pcthispanic':test[\"pcthispanic\"].values.tolist(), 'gender':test[\"gender\"].values.tolist(),                              \n",
    "        'display_name_words':test[\"display_name_words\"].values.tolist(), 'display_name_len':test[\"display_name_len\"].values.tolist(),\n",
    "        'display_name_upper':test[\"display_name_upper\"].values.tolist(), 'display_name_lower':test[\"display_name_lower\"].values.tolist(),\n",
    "        'Follower_Count':test[\"Follower_Count\"].values.tolist(), 'Verified':test[\"Verified\"].values.tolist(),                                  \n",
    "        'Location':test[\"Location\"].values.tolist(), 'is_location':test[\"is_location\"].values.tolist(), 'Description':test[\"Description\"].values.tolist(), \n",
    "        'is_description':test[\"is_description\"].values.tolist(), 'Friends_Count':test[\"Friends_Count\"].values.tolist(), 'List_Count':test[\"List_Count\"].values.tolist(),                               \n",
    "        'Favourites_Count':test[\"Favourites_Count\"].values.tolist(), 'Statuses_Count':test[\"Statuses_Count\"].values.tolist(), 'Time Zone':test[\"Time_Zone\"].values.tolist(),\n",
    "        'is_timezone':test[\"is_timezone\"].values.tolist(), 'Language':test[\"Language\"].values.tolist(), 'is_Background_Image':test[\"is_Background_Image\"].values.tolist(),                              \n",
    "        'Default_Profile':test[\"Default_Profile\"].values.tolist(), 'Default_Profile_Image':test[\"Default_Profile_Image\"].values.tolist(),\n",
    "        'has_url':test[\"has_url\"].values.tolist(),\n",
    "        'Year_created':test[\"Year_created\"].values.tolist(), \n",
    "        'Profile_Background_Tile':test[\"Profile_Background_Tile\"].values.tolist(), 'Account_Status':test[\"Account_Status\"].values.tolist(),\n",
    "        'is_active':test[\"is_active\"].values.tolist(), 'username_len':test[\"username_len\"].values.tolist(), \n",
    "        'user_name_upper':test[\"user_name_upper\"].values.tolist(), 'user_name_lower':test[\"user_name_lower\"].values.tolist(), \n",
    "        'faces_detected':test[\"faces_detected\"].values.tolist(), \n",
    "        'is_geo':test[\"is_geo\"].values.tolist(),  \n",
    "        'total_misspelling':test[\"total_misspelling\"].values.tolist(), 'total_correct':test[\"total_correct\"].values.tolist(),\n",
    "        'contains_misspellings':test[\"contains_misspellings\"].values.tolist(), 'profanity_count':test[\"profanity_count\"].values.tolist(),\n",
    "        'not_profanity':test[\"not_profanity\"].values.tolist(),                               \n",
    "        'contains_profanity':test[\"contains_profanity\"].values.tolist(), 'subjectivity':test[\"subjectivity\"].values.tolist(),\n",
    "        'word_count':test[\"word_count\"].values.tolist(), 'tweet_len':test[\"tweet_len\"].values.tolist(), 'tweet_len_cat':test[\"tweet_len_cat\"].values.tolist(), \n",
    "        'period_count':test[\"period_count\"].values.tolist(), 'period_percent':test[\"period_percent\"].values.tolist(),\n",
    "        'qmark_count':test[\"qmark_count\"].values.tolist(), 'qmark_percent':test[\"qmark_percent\"].values.tolist(),                               \n",
    "        'upper_count':test[\"upper_count\"].values.tolist(), 'upper_percent':test[\"upper_percent\"].values.tolist(), \n",
    "        'lower_count':test[\"lower_count\"].values.tolist(), 'lower_percent':test[\"lower_percent\"].values.tolist(), \n",
    "        'comma_count':test[\"comma_count\"].values.tolist(), 'comma_percent':test[\"comma_percent\"].values.tolist(),                               \n",
    "        'flash_count':test[\"flash_count\"].values.tolist(), 'flash_norm':test[\"flash_norm\"].values.tolist(),                               \n",
    "        'exclamation_count':test[\"exclamation_count\"].values.tolist(), 'exclamation_percent':test[\"exclamation_percent\"].values.tolist(),                               \n",
    "        'is_url':test[\"is_url\"].values.tolist(), 'url_len':test[\"url_len\"].values.tolist(),                               \n",
    "        'hashtag_len':test[\"hashtag_len\"].values.tolist(), 'hashtag_count':test[\"hashtag_count\"].values.tolist(),                               \n",
    "        'mention_count':test[\"mention_count\"].values.tolist(), 'mention_len':test[\"mention_len\"].values.tolist(),\n",
    "        'is_reply':test[\"is_reply\"].values.tolist(), 'Overall.score.EMOTIVE':test[\"Overall.score.EMOTIVE\"].values.tolist(),\n",
    "        'Anger':test[\"Anger\"].values.tolist(),\n",
    "        'Anger_norm':test[\"Anger_norm\"].values.tolist(), 'Confusion':test[\"Confusion\"].values.tolist(),\n",
    "        'Confusion_norm':test[\"Confusion_norm\"].values.tolist(), 'Disgust':test[\"Disgust\"].values.tolist(),\n",
    "        'Disgust_norm':test[\"Disgust_norm\"].values.tolist(),\n",
    "        'Fear':test[\"Fear\"].values.tolist(), 'Fear_norm':test[\"Fear_norm\"].values.tolist(),\n",
    "        'Happiness':test[\"Happiness\"].values.tolist(), 'Happiness_norm':test[\"Happiness_norm\"].values.tolist(),\n",
    "        'Sadness':test[\"Sadness\"].values.tolist(), 'Sadness_norm':test[\"Sadness_norm\"].values.tolist(),                              \n",
    "        'Shame':test[\"Shame\"].values.tolist(), 'Shame_norm':test[\"Shame_norm\"].values.tolist(), \n",
    "        'Surprise':test[\"Surprise\"].values.tolist(), 'Surprise_norm':test[\"Surprise_norm\"].values.tolist(),                                \n",
    "        'is_Overall.score.EMOTIVE':test[\"is_Overall.score.EMOTIVE\"].values.tolist(), 'is_Anger':test[\"is_Anger\"].values.tolist(),\n",
    "        'is_Confusion':test[\"is_Confusion\"].values.tolist(), 'is_Disgust':test[\"is_Disgust\"].values.tolist(),                              \n",
    "        'is_Fear':test[\"is_Fear\"].values.tolist(), 'is_Happiness':test[\"is_Happiness\"].values.tolist(),                              \n",
    "        'is_Sadness':test[\"is_Sadness\"].values.tolist(), 'is_Shame':test[\"is_Shame\"].values.tolist(),                              \n",
    "        'is_Surprise':test[\"is_Surprise\"].values.tolist(), 'Type':test[\"Type\"].values.tolist(),                              \n",
    "        'senti_positive':test[\"senti_positive\"].values.tolist(), \n",
    "        'senti_mix':test[\"senti_mix\"].values.tolist(), 'total_senti':test[\"total_senti\"].values.tolist(),\n",
    "        'senti_positive_percent':test[\"senti_positive_percent\"].values.tolist(), 'senti_negative_percent':test[\"senti_negative_percent\"].values.tolist(),\n",
    "        'CC':test[\"CC\"].values.tolist(), 'CD':test[\"CD\"].values.tolist(), \n",
    "        'DT':test[\"DT\"].values.tolist(), 'EX':test[\"EX\"].values.tolist(), \n",
    "        'FW':test[\"FW\"].values.tolist(), 'IN':test[\"IN\"].values.tolist(), \n",
    "        'JJ':test[\"JJ\"].values.tolist(), 'JJR':test[\"JJR\"].values.tolist(), \n",
    "        'JJS':test[\"JJS\"].values.tolist(), 'MD':test[\"MD\"].values.tolist(), \n",
    "        'NN':test[\"NN\"].values.tolist(), 'NNP':test[\"NNP\"].values.tolist(), \n",
    "        'NNPS':test[\"NNPS\"].values.tolist(), 'NNS':test[\"NNS\"].values.tolist(),                               \n",
    "        'PDT':test[\"PDT\"].values.tolist(), 'POS':test[\"POS\"].values.tolist(), \n",
    "        'PRP':test[\"PRP\"].values.tolist(), 'PRP$':test[\"PRP$\"].values.tolist(),  \n",
    "        'RBR':test[\"RBR\"].values.tolist(), \n",
    "        'RBS':test[\"RBS\"].values.tolist(), 'RP':test[\"RP\"].values.tolist(),                               \n",
    "        'SYM':test[\"SYM\"].values.tolist(), 'TO':test[\"TO\"].values.tolist(), \n",
    "        'UH':test[\"UH\"].values.tolist(), 'VB':test[\"VB\"].values.tolist(),                                \n",
    "        'VBD':test[\"VBD\"].values.tolist(), 'VBG':test[\"VBG\"].values.tolist(), \n",
    "        'VBN':test[\"VBN\"].values.tolist(), 'VBP':test[\"VBP\"].values.tolist(),                               \n",
    "        'VBZ':test[\"VBZ\"].values.tolist(), 'WDT':test[\"WDT\"].values.tolist(),\n",
    "        'WP':test[\"WP\"].values.tolist(), 'WP$':test[\"WP$\"].values.tolist(), \n",
    "        'WRB':test[\"WRB\"].values.tolist(), \n",
    "    \n",
    "                            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scalar = RobustScaler()\n",
    "\n",
    "#Scale Training Set\n",
    "otherfeaturestraindf[['Hour', 'pctwhite', 'pctblack', 'pctapi', 'pctaian', 'pct2prace', 'display_name_words', 'display_name_len', 'display_name_upper', 'display_name_lower', 'Follower_Count', \n",
    "              'Friends_Count', 'List_Count', 'Favourites_Count', 'Statuses_Count', 'Year_created', 'username_len', 'user_name_upper', 'user_name_lower', 'total_misspelling', 'total_correct',\n",
    "            'profanity_count', 'not_profanity', 'subjectivity', 'word_count', 'tweet_len', 'period_count', 'period_percent', 'qmark_count', 'qmark_percent', \n",
    "              'upper_count', 'upper_percent', 'lower_count', 'lower_percent', 'comma_count', 'comma_percent', 'fslash_count', 'flash_norm', 'exclamation_count', 'exclamation_percent', \n",
    "              'tweet_len_cat', 'url_len', 'hashtag_count', 'hashtag_len', 'mention_count', 'mention_len', 'Overall.score.EMOTIVE', 'is_Overall.score.EMOTIVE', 'Anger', 'Anger_norm', 'Confusion', 'Confusion_norm', 'Disgust',\n",
    "              'Disgust_norm', 'Fear', 'Fear_norm', 'Happiness', 'Happiness_norm', 'Sadness', 'Sadness_norm', 'Shame', 'Shame_norm', 'Surprise', 'Surprise_norm', 'senti_positive', \n",
    "              'senti_mix', 'total_senti', 'senti_positive_percent', 'senti_negative_percent', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS',\n",
    "              'PRP', 'PRP$', 'ADV', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']] = scalar.fit_transform(otherfeaturestraindf[['Hour', 'pctwhite', 'pctblack', 'pctapi', 'pctaian', 'pct2prace', 'display_name_words', 'display_name_len', 'display_name_upper', 'display_name_lower', 'Follower_Count', \n",
    "              'Friends_Count', 'List_Count', 'Favourites_Count', 'Statuses_Count', 'Year_created', 'username_len', 'user_name_upper', 'user_name_lower', 'total_misspelling', 'total_correct',\n",
    "            'profanity_count', 'not_profanity', 'subjectivity', 'word_count', 'tweet_len', 'period_count', 'period_percent', 'qmark_count', 'qmark_percent', \n",
    "              'upper_count', 'upper_percent', 'lower_count', 'lower_percent', 'comma_count', 'comma_percent', 'fslash_count', 'flash_norm', 'exclamation_count', 'exclamation_percent',  \n",
    "              'tweet_len_cat', 'url_len', 'hashtag_count', 'hashtag_len', 'mention_count', 'mention_len', 'Overall.score.EMOTIVE', 'is_Overall.score.EMOTIVE', 'Anger', 'Anger_norm', 'Confusion', 'Confusion_norm', 'Disgust',\n",
    "              'Disgust_norm', 'Fear', 'Fear_norm', 'Happiness', 'Happiness_norm', 'Sadness', 'Sadness_norm', 'Shame', 'Shame_norm', 'Surprise', 'Surprise_norm', 'senti_positive', \n",
    "              'senti_mix', 'total_senti', 'senti_positive_percent', 'senti_negative_percent', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS',\n",
    "              'PRP', 'PRP$', 'ADV', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']])\n",
    "\n",
    "#Scale Test Set\n",
    "otherfeaturestestdf[['Hour', 'pctwhite', 'pctblack', 'pctapi', 'pctaian', 'pct2prace', 'display_name_words', 'display_name_len', 'display_name_upper', 'display_name_lower', 'Follower_Count', \n",
    "              'Friends_Count', 'List_Count', 'Favourites_Count', 'Statuses_Count', 'Year_created', 'username_len', 'user_name_upper', 'user_name_lower', 'total_misspelling', 'total_correct',\n",
    "              'profanity_count', 'not_profanity', 'subjectivity', 'word_count', 'tweet_len', 'period_count', 'period_percent', 'qmark_count', 'qmark_percent', \n",
    "              'upper_count', 'upper_percent', 'lower_count', 'lower_percent', 'comma_count', 'comma_percent', 'fslash_count', 'flash_norm', 'exclamation_count', 'exclamation_percent', \n",
    "              'tweet_len_cat', 'url_len', 'hashtag_count', 'hashtag_len', 'mention_count', 'mention_len', 'Overall.score.EMOTIVE', 'is_Overall.score.EMOTIVE', 'Anger', 'Anger_norm', 'Confusion', 'Confusion_norm', 'Disgust',\n",
    "              'Disgust_norm', 'Fear', 'Fear_norm', 'Happiness', 'Happiness_norm', 'Sadness', 'Sadness_norm', 'Shame', 'Shame_norm', 'Surprise', 'Surprise_norm', 'senti_positive', \n",
    "              'senti_mix', 'total_senti', 'senti_positive_percent', 'senti_negative_percent', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS',\n",
    "              'PRP', 'PRP$', 'ADV', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']] = scalar.transform(otherfeaturestestdf[['Hour', 'pctwhite', 'pctblack', 'pctapi', 'pctaian', 'pct2prace', 'display_name_words', 'display_name_len', 'display_name_upper', 'display_name_lower', 'Follower_Count', \n",
    "              'Friends_Count', 'List_Count', 'Favourites_Count', 'Statuses_Count', 'Year_created', 'username_len', 'user_name_upper', 'user_name_lower', 'total_misspelling', 'total_correct',\n",
    "              'profanity_count', 'not_profanity', 'subjectivity', 'word_count', 'tweet_len', 'period_count', 'period_percent', 'qmark_count', 'qmark_percent', \n",
    "              'upper_count', 'upper_percent', 'lower_count', 'lower_percent', 'comma_count', 'comma_percent', 'fslash_count', 'flash_norm', 'exclamation_count', 'exclamation_percent', \n",
    "              'tweet_len_cat', 'url_len', 'hashtag_count', 'hashtag_len', 'mention_count', 'mention_len', 'Overall.score.EMOTIVE', 'is_Overall.score.EMOTIVE', 'Anger', 'Anger_norm', 'Confusion', 'Confusion_norm', 'Disgust',\n",
    "              'Disgust_norm', 'Fear', 'Fear_norm', 'Happiness', 'Happiness_norm', 'Sadness', 'Sadness_norm', 'Shame', 'Shame_norm', 'Surprise', 'Surprise_norm', 'senti_positive', \n",
    "              'senti_mix', 'total_senti', 'senti_positive_percent', 'senti_negative_percent', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS',\n",
    "              'PRP', 'PRP$', 'ADV', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print \"Parsing train reviews...\"\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train['tweet_text']:\n",
    "\tclean_train_reviews.append( \" \".join( KaggleWord2VecUtility.review_to_wordlist( review )))\n",
    "\n",
    "print \"Parsing test reviews...\"\n",
    "\n",
    "clean_test_reviews = []\n",
    "for review in test['tweet_text']:\n",
    "\tclean_test_reviews.append( \" \".join( KaggleWord2VecUtility.review_to_wordlist( review )))\n",
    "\n",
    "    \n",
    "#\n",
    "\n",
    "\n",
    "#vectorizer = CountVectorizer( max_features = 100, ngram_range = ( 1, 3 ), \n",
    "#\tsublinear_tf = True, stop_words = 'english' )\n",
    "\n",
    "vectorizer = TfidfVectorizer( \n",
    "\tuse_idf = True, ngram_range = (1, 3), norm = \"l1\", min_df=5, max_df=50  )\n",
    "\n",
    "class RowIterator(TransformerMixin):\n",
    "    \"\"\" Prepare dataframe for DictVectorizer \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (row for _, row in X.iterrows())\n",
    "\n",
    "\n",
    "vectorizerb = make_pipeline(RowIterator(), DictVectorizer())\n",
    "\n",
    "train_word_features = vectorizer.fit_transform( clean_train_reviews ).toarray()\n",
    "test_word_features = vectorizer.transform( clean_test_reviews ).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_features_train_vector = vectorizerb.fit_transform( otherfeaturestraindf )\n",
    "other_features_test_vector = vectorizerb.transform ( otherfeaturestestdf )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Combine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Non PCA\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "train_data_features = hstack([other_features_train_vector,train_word_features]).toarray()\n",
    "test_data_features = hstack([other_features_test_vector,test_word_features]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectFromModel\n",
    "\n",
    "selector = SelectKBest(mutual_info_classif, k=300)\n",
    "\n",
    "\n",
    "train_data_features_engineered = selector.fit_transform(train_data_features, train[\"Soft\"].values)\n",
    "\n",
    "test_data_features_engineered = selector.transform(test_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#Undersampling\n",
    "#rus = RandomUnderSampler(ratio='majority', replacement=True)\n",
    "#x_train_res, x_train_res = rus.fit_sample(train_data_features_engineered, train[\"Remain\"].values)\n",
    "\n",
    "\n",
    "#SMOTE\n",
    "sm = SMOTE(random_state=12)\n",
    "x_train_res, y_train_res = sm.fit_sample(train_data_features, train[\"Soft\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier as CLF\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def train_and_eval_auc( model, train_x, train_y, test_x, test_y ):\n",
    "    global acc\n",
    "    global fscore\n",
    "    global rec\n",
    "    global prec\n",
    "    model.fit( train_x, train_y )\n",
    "    p = model.predict_proba( test_x )\n",
    "    pacc = model.predict( test_x )\n",
    "    auc = AUC( test_y, p[:,1] )\n",
    "    acc = accuracy_score(test_y, pacc)\n",
    "    fscore = f1_score(test_y, pacc)\n",
    "    rec = recall_score(test_y, pacc)\n",
    "    prec = precision_score(test_y, pacc)\n",
    "    \n",
    "    preds = clf.predict_proba(test_x)[:,1]\n",
    "    fpr, tpr, _ = metrics.roc_curve(test_y, preds)\n",
    "\n",
    "    cnf_matrix = confusion_matrix(test_y, pacc)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=[\"Other\",\"Supports Soft Brexit\"],\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.show()\n",
    "    return auc\n",
    "\n",
    "clf=CLF(n_estimators=100,max_depth=9, min_samples_split=400, max_features=90, min_samples_leaf=6, random_state=44)\n",
    "auc = train_and_eval_auc( clf, train_data_features, train[\"Soft\"].values,\n",
    "\ttest_data_features, test[\"Soft\"].values )\n",
    "print \"AUC:\", auc\n",
    "print \"Accuracy:\", acc\n",
    "print \"F1 Score:\", fscore\n",
    "print \"Recall:\", rec\n",
    "print \"Precision:\", prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    ">>> joblib.dump(clf, 'Soft.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "xval = cross_val_score(clf, train_data_features, train[\"Soft\"], cv=10, scoring='roc_auc')\n",
    "print xval\n",
    "print xval.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Informative Features (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "def show_most_informative_features(vectorizer, lr, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print \"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2)\n",
    "        \n",
    "show_most_informative_features(vectorizer, lr, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test1 = {'n_estimators':range(300,410,10)}\n",
    "gsearch1 = GridSearchCV(estimator = CLF(max_features='sqrt', random_state=44), \n",
    "param_grid = param_test1, scoring='f1',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(train_data_features,train[\"Soft\"])\n",
    "\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test2 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,200)}\n",
    "gsearch2 = GridSearchCV(estimator = CLF(n_estimators=100, max_features='sqrt', random_state=44), \n",
    "param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch2.fit(train_data_features,train[\"Soft\"])\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test3 = {'min_samples_leaf':range(4,8,1)}\n",
    "gsearch3 = GridSearchCV(estimator = CLF(n_estimators=100,max_depth=9, min_samples_split=400, max_features='sqrt', random_state=44), \n",
    "param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch3.fit(train_data_features,train[\"Soft\"])\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test4 = {'max_features':range(80,96,5)}\n",
    "gsearch4 = GridSearchCV(estimator = CLF(n_estimators=100,max_depth=9, min_samples_split=400, min_samples_leaf=6, random_state=44),\n",
    "param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch4.fit(train_data_features,train[\"Soft\"])\n",
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test5 = {'subsample':[0.85,0.9, 0.95, 1]}\n",
    "gsearch5 = GridSearchCV(estimator = CLF(n_estimators=100,max_depth=9, min_samples_split=400, max_features=90, min_samples_leaf=6, random_state=44),\n",
    "param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch5.fit(train_data_features,train[\"Soft\"])\n",
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
